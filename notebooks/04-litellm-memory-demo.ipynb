{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory with LiteLLM\n",
    "\n",
    "This notebook demonstrates how to add persistent memory to any LLM app using the `hindsight-litellm` package. Memory storage and injection happen automatically via LiteLLM callbacks - no manual memory management needed!\n",
    "\n",
    "**Key features demonstrated:**\n",
    "1. `configure()` + `enable()` - Set up automatic memory integration\n",
    "2. Automatic storage - Conversations are stored after each LLM call\n",
    "3. Automatic injection - Relevant memories are injected into prompts\n",
    "\n",
    "The `hindsight-litellm` package hooks into LiteLLM's callback system to:\n",
    "- Store each conversation after successful LLM responses\n",
    "- Inject relevant memories into the system prompt before LLM calls\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have Hindsight running:\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-key\n",
    "\n",
    "docker run --rm -it --pull always -p 8888:8888 -p 9999:9999 \\\n",
    "  -e HINDSIGHT_API_LLM_API_KEY=$OPENAI_API_KEY \\\n",
    "  -e HINDSIGHT_API_LLM_MODEL=o3-mini \\\n",
    "  -v $HOME/.hindsight-docker:/home/hindsight/.pg0 \\\n",
    "  ghcr.io/vectorize-io/hindsight:latest\n",
    "```\n",
    "\n",
    "- API: http://localhost:8888\n",
    "- UI: http://localhost:9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hindsight-litellm litellm nest_asyncio python-dotenv -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Apply nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"LiteLLM Router\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"LiteLLM Proxy\").setLevel(logging.WARNING)\n",
    "\n",
    "# Import hindsight_litellm\n",
    "import hindsight_litellm\n",
    "\n",
    "# Configuration\n",
    "HINDSIGHT_API_URL = os.getenv(\"HINDSIGHT_API_URL\", \"http://localhost:8888\")\n",
    "\n",
    "# Check for API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Warning: OPENAI_API_KEY not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Enable Automatic Memory\n",
    "\n",
    "This is all you need! After this, all LiteLLM calls will automatically:\n",
    "- Have relevant memories injected into the prompt\n",
    "- Store conversations to Hindsight after the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique bank_id for this demo session\n",
    "bank_id = f\"demo-{uuid.uuid4().hex[:8]}\"\n",
    "print(f\"Using bank_id: {bank_id}\")\n",
    "\n",
    "# Configure and enable hindsight\n",
    "hindsight_litellm.configure(\n",
    "    hindsight_api_url=HINDSIGHT_API_URL,\n",
    "    bank_id=bank_id,\n",
    "    store_conversations=True,  # Automatically store conversations\n",
    "    inject_memories=True,       # Automatically inject relevant memories\n",
    "    verbose=True,               # Enable logging to debug memory operations\n",
    ")\n",
    "hindsight_litellm.enable()\n",
    "\n",
    "print(\"Hindsight memory integration enabled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation 1: User Introduces Themselves\n",
    "\n",
    "In this first conversation, the user shares some information about themselves. This will be automatically stored to Hindsight memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_1 = \"Hi! I'm Alex and I work at Google as a software engineer. I love Python and machine learning.\"\n",
    "print(f\"User: {user_message_1}\\n\")\n",
    "\n",
    "# Use hindsight_litellm.completion() directly\n",
    "response_1 = hindsight_litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message_1}\n",
    "    ],\n",
    ")\n",
    "\n",
    "assistant_response_1 = response_1.choices[0].message.content\n",
    "print(f\"Assistant: {assistant_response_1}\")\n",
    "print(\"\\n(Conversation automatically stored to Hindsight)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Memory Processing\n",
    "\n",
    "Hindsight needs a few seconds to process and extract facts from the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting 12 seconds for memory processing...\")\n",
    "time.sleep(12)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation 2: Test Memory-Augmented Response\n",
    "\n",
    "Now we start a fresh conversation and ask what the assistant remembers. The memories from the previous conversation will be automatically injected into the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_2 = \"What do you know about me? What programming language should I use for my next project?\"\n",
    "print(f\"User: {user_message_2}\\n\")\n",
    "\n",
    "# Memories are automatically injected before this call!\n",
    "response_2 = hindsight_litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message_2}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {response_2.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The assistant should have remembered that Alex:\n",
    "- Works at Google as a software engineer\n",
    "- Loves Python and machine learning\n",
    "\n",
    "And it should have recommended Python based on that knowledge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memories stored in bank: {bank_id}\")\n",
    "print(f\"View in UI: http://localhost:9999/banks/{bank_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindsight_litellm.cleanup()\n",
    "\n",
    "# Optional: delete the bank\n",
    "import requests\n",
    "response = requests.delete(f\"{HINDSIGHT_API_URL}/v1/default/banks/{bank_id}\")\n",
    "print(f\"Deleted bank: {response.json()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "tags": {
   "sdk": "hindsight-litellm",
   "topic": "Quick Start"
  },
  "description": "Add automatic memory to any LLM app using LiteLLM callbacks"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}